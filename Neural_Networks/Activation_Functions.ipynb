{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions \n",
    "---\n",
    "\n",
    "## Why they are needed?\n",
    "\n",
    "- If a neuron's output is not bounded, so it cannot decide whether it should \"fire\" or not. (Brains work by firing some neurons)\n",
    "\n",
    "- Activation functions check whether the weighted sum computed by the neuron is enough to make the neuron available to preceding layers.\n",
    "\n",
    "- Without the use of activation functions, the neurons would always compute a **linear combination** of the inputs, no matter how many hidden layers are there. It is essentially becomes like linear regression.\n",
    "\n",
    "- It is used to introduce a \"little non-linearity\" into the network. It makes the network more powerful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Activation functions\n",
    "\n",
    "### <ins>Binary Step Function</ins> -\n",
    "\n",
    "- Same as declaring a threshold value. If neuron output is greater than the threshold than activate the neuron, otherwise not.\n",
    "- Can work is using for a *binary classifier*.\n",
    "\n",
    "**Disadvantage** \n",
    "\n",
    "- Since the gradient of the step function is zero, in the backpropagation process it causes difficulty in learning process, i.e, the weights and biases will not be updated.\n",
    "- A neuron can be activated or not activated, there are no intermediate value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins>Linear Function</ins> -\n",
    "- Activation is proportional to the inputs.\n",
    "- Derivative exists. Equal to the coefficient of the input.\n",
    "\n",
    "**Disadvantage** \n",
    "\n",
    "- Although the gradient is not zero, it is a constant value. It does not depend upon the input. Therefore, the weights and biases will be updated during the learning process but with the same factor \n",
    "- The gradient will be same for every iteration, so complex patterns from data cannot be captured.\n",
    "- Same problem, as it is a linear function, the output will also be a linear function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins>Sigmoid Function</ins> -\n",
    "- One of the most popular choice.\n",
    "- It is a non-linear function. Therefore, outputs of the neurons will be non-linear.\n",
    "- It is continuously differentiable. \n",
    "\n",
    "**Disadvantage** \n",
    "\n",
    "- The gradient value changes from -3 to +3, but flattens out in other regions, which implies that the gradient is close to zero and the network is not learning. (Vanishing gradient problem)\n",
    "- Function is not symmetric around zero. The output of all the neurons will be of same sign."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins>Tanh Function</ins> -\n",
    "\n",
    "- Preferred over sigmoid function.\n",
    "- Similar to sigmoid, but it is symmetric around origin.\n",
    "- Ranges from -1 to +1.\n",
    "- Continuously differentiable. The gradients are steeper than sigmoid function.\n",
    "\n",
    "**Disadvantage** \n",
    "\n",
    "- Same vanishing gradient problem as sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins>ReLU Function</ins> -\n",
    "\n",
    "- Non-linear function. Stands for the **Rectified Linear Unit**.\n",
    "- Advantage of ReLU over other functions is that at one time only some of the neurons will be activated making it efficient computationally.\n",
    "\n",
    "**Disadvantage** \n",
    "- the gradient for the negative side is zero, so it could create dead neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
