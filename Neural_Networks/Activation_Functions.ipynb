{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions \n",
    "---\n",
    "\n",
    "## Why they are needed?\n",
    "\n",
    "- If a neuron's output is not bounded, it cannot decide whether it should \"fire\" or not. (Brains work by firing some neurons)\n",
    "\n",
    "- Activation functions check whether the weighted sum computed by the neuron is enough to make the neuron available to preceding layers.\n",
    "\n",
    "- Without the use of activation functions, the neurons would always compute a **linear combination** of the inputs, no matter how many hidden layers are there. It will essentially become like linear regression.\n",
    "\n",
    "- It is used to introduce a \"little non-linearity\" into the network. It makes the network more powerful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Activation functions\n",
    "\n",
    "### <ins>Binary Step Function</ins> -\n",
    "![step function](https://www.saedsayad.com/images/ANN_Unit_step.png)\n",
    "- Same as declaring a threshold value. If neuron output is greater than the threshold than activate the neuron, otherwise not.\n",
    "- Can work for a *binary classifier*.\n",
    "\n",
    "**Disadvantage** \n",
    "\n",
    "- Since the gradient of the step function is zero, in the backpropagation process it causes difficulty in learning process, i.e, the weights and biases will not be updated.\n",
    "- A neuron can be activated or not activated, there are no intermediate value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins>Linear Function</ins> -\n",
    "![Linear function](https://www.saedsayad.com/images/LinearFunction.png)\n",
    "\n",
    "- Activation is proportional to the inputs.\n",
    "- Derivative exists. Equal to the coefficient of the input.\n",
    "\n",
    "**Disadvantage** \n",
    "\n",
    "- Although the gradient is not zero, it is a constant value. It does not depend upon the input. Therefore, the weights and biases will be updated during the learning process but with the same factor \n",
    "- The gradient will be same for every iteration, so complex patterns from data cannot be captured.\n",
    "- Same problem, as it is a linear function, the output will also be a linear function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins>Sigmoid Function</ins> -\n",
    "![Sigmoid](https://www.saedsayad.com/images/ANN_Sigmoid.png)\n",
    "\n",
    "- One of the most popular choice.\n",
    "- It is a non-linear function. Therefore, outputs of the neurons will be non-linear.\n",
    "- It is continuously differentiable. \n",
    "\n",
    "**Disadvantage** \n",
    "\n",
    "- The gradient value changes from -3 to +3, but flattens out in other regions, which implies that the gradient is close to zero and the network is not learning. (Vanishing gradient problem)\n",
    "- Function is not symmetric around zero. The output of all the neurons will be of same sign."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins>Tanh Function</ins> -\n",
    "![tanh function](https://cdn-images-1.medium.com/freeze/max/1000/1*1It8846pzYayiC0G_7FIBA.png?q=20)\n",
    "\n",
    "- Preferred over sigmoid function.\n",
    "- Similar to sigmoid, but it is symmetric around origin.\n",
    "- Ranges from -1 to +1.\n",
    "- Continuously differentiable. The gradients are steeper than sigmoid function.\n",
    "\n",
    "**Disadvantage** \n",
    "\n",
    "- Same vanishing gradient problem as sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins>ReLU Function</ins> -\n",
    "![ReLU](https://cdn-images-1.medium.com/freeze/max/1000/1*TbZnkZYI5vwOQGUBd6uXAQ.png?q=20)\n",
    "- Non-linear function. Stands for the **Rectified Linear Unit**.\n",
    "- Advantage of ReLU over other functions is that at one time only some of the neurons will be activated making it efficient computationally. The network also becomes lighter.\n",
    "\n",
    "**Disadvantage** \n",
    "- The gradient for the negative side is zero, so it could create dead neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins>Leaky ReLU Function</ins> -\n",
    "![leaky relu](https://www.i2tutorials.com/wp-content/uploads/2019/09/Deep-learning-25-i2tutorials.png)\n",
    "- Improved version of ReLU function.\n",
    "- Since the gradient of negative inputs becomes zero and causes dead neurons, we give a small linear component of x to the output.\n",
    "- No dead neurons can form because of the small linear component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins>Parameterised ReLU Function</ins> -\n",
    "![parametric relu](https://linzhouhan.files.wordpress.com/2015/04/prelu.png)\n",
    "\n",
    "- Another variant of ReLU function.\n",
    "- A new parameter acts as the slope of the negative part of the function.\n",
    "- Here, the parameter becomes trainable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Courtesy -\n",
    "\n",
    "- https://www.saedsayad.com/artificial_neural_network.htm\n",
    "- https://mc.ai/activation-functions-in-neural-networks/\n",
    "- https://www.i2tutorials.com/explain-step-threshold-and-leaky-relu-activation-functions/\n",
    "- https://linzhouhan.files.wordpress.com/2015/04/prelu.png"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
