{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving neural network learning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The cross-entropy cost function\n",
    "---\n",
    "> We learn slowly when our errors are less well-defined.\n",
    "\n",
    "The derivative of the quadratic cost function is -\n",
    "\n",
    "$\n",
    "\\begin{eqnarray} \n",
    "  \\frac{\\partial C}{\\partial w} & = & (a-y)\\sigma'(z) x = a \\sigma'(z) \\tag{1}\\\\\n",
    "  \\frac{\\partial C}{\\partial b} & = & (a-y)\\sigma'(z) = a \\sigma'(z),\n",
    "\\tag{2}\\end{eqnarray}\n",
    "$\n",
    "**Note** - First differentiate $C$ w.r.t $a$, then $a$ w.r.t $z$, then $z$ w.r.t $w$ or $b$.\n",
    "\n",
    "\n",
    "**Origin of slow learning** - If the input values to the neuron are too large or too small, then the sigmoid function curve gets very flat, so the derivative of sigma gets very small.\n",
    "This can be solved by using a different cost function rather than the quadratic cost function.\n",
    "\n",
    "The cross-entropy function is defined as \n",
    "$\n",
    "\\begin{eqnarray} \n",
    "  C = -\\frac{1}{n} \\sum_x \\left[y \\ln a + (1-y ) \\ln (1-a) \\right],\n",
    "\\tag{A}\\end{eqnarray}\n",
    "$\n",
    "where n is the total number of training data, sum over all the training inputs, x and output y.\n",
    "\n",
    "Cross-entropy as cost function --\n",
    "\n",
    "- Non-negative - All individual terms are negative, since the `log` is in range of 0 to 1. There is also the negative sign out front.\n",
    "- If the actual output is close to desired output for all training inputs, $C \\approx 0$\n",
    "\n",
    "$C$ tends towards zero as the neuron gets better at computing the desired output. It has the benefit over quadratic cost function that it avoids the slow learning problem.\n",
    "Computing the partial derivative of the cross-entropy cost with respect to the weights.\n",
    "$\n",
    "\\begin{eqnarray}\n",
    "  \\frac{\\partial C}{\\partial w_j} & = & -\\frac{1}{n} \\sum_x \\left(\n",
    "    \\frac{y }{\\sigma(z)} -\\frac{(1-y)}{1-\\sigma(z)} \\right)\n",
    "  \\frac{\\partial \\sigma}{\\partial w_j} \\tag{3}\\\\\n",
    " & = & -\\frac{1}{n} \\sum_x \\left( \n",
    "    \\frac{y}{\\sigma(z)} \n",
    "    -\\frac{(1-y)}{1-\\sigma(z)} \\right)\\sigma'(z) x_j.\n",
    "\\tag{4}\\end{eqnarray}\n",
    "$\n",
    "simplifying and canceling gives us\n",
    "\n",
    "$\n",
    "\\begin{eqnarray} \n",
    "  \\frac{\\partial C}{\\partial w_j} =  \\frac{1}{n} \\sum_x x_j(\\sigma(z)-y).\n",
    "\\tag{4}\\end{eqnarray}\n",
    "$\n",
    "\n",
    "It tells us that the rate at which the wight learns is controlled by $\\sigma(z)-y$, i.e. error in the output. Larger the error, faster the neuron will learn.\n",
    "When we use cross entropy cost function,  $\\sigma'(z)$ gets canceled out so slow learning will not be a problem. This cancellation is the special miracle ensured by the cross-entropy cost function.\n",
    "\n",
    "The cost function for all the neurons in the output layer\n",
    "$\n",
    "\\begin{eqnarray}  C = -\\frac{1}{n} \\sum_x\n",
    "  \\sum_j \\left[y_j \\ln a^L_j + (1-y_j) \\ln (1-a^L_j) \\right].\n",
    "\\tag{B}\\end{eqnarray}\n",
    "$\n",
    "\n",
    "- It is the generalization of the cross-entropy for probability distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where does cross-entropy comes from\n",
    "\n",
    "To-Do later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting and regularization\n",
    "---\n",
    "\n",
    "Models with large number of free parameters can describe an amazingly wide range of phenomena. Even if a model agrees well with the available data, that doesn't make it a good model. It may mean there is enough freedom in the model that it can describe almost any dataset of given size without capturing any genuine insights into the underlying phenomenon. Model then fails to generalize to new situations. So, how can we trust the results of neural network when it contains hundreds and thousands of parameters.\n",
    "\n",
    "During the training of a model, the cost might appear to decrease after every epoch but the accuracy might not improve much after certain epoch of training, which indicates that the model fails to generalize after certain steps. The cost improvement is an illusion. It is called as `overfitting` or `overtraining`.\n",
    "\n",
    "Very high test accuracy also hints at the possibility of overfitting of the model.\n",
    "The obvious way to detect overfitting is to keep track of accuracy on the test data when the network trains. If the accuracy of the test data is no longer improving, then we should stop the training. We can use a `validation set` to test our model first for signs of overfitting.\n",
    "\n",
    "**Early Stopping** - We compute the classification accuracy on the `validation data`, once it is saturated, we stop the training.\n",
    "\n",
    "Why use the `validation data` rather than `test data` to prevent overfitting?\n",
    "\n",
    "It is part of a more general stategy, which is used to evaluate different trial choices of hyper-parameters such as epochs, learning rate and so on.\n",
    "There are many different choices of hyper-parameters. If we set the hyper-parameters based on the evaluations on the `test data` it's possible we may end up overfitting the hyper-parameters to the `test data`. We guard against that by figuring out the hyper-parameters using the `validation data`. Then it gives the true measure of generalization on the `test data`.\n",
    "\n",
    "- Also, adding more `training data` can help to reduce overfitting, but it is expensive and not practical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
