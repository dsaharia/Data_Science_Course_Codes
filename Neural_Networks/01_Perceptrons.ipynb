{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptrons\n",
    "---\n",
    "\n",
    "- It is a type of artificial neuron, developed in the 50s by Frank Rosenblatt.\n",
    "- Used to understand **Sigmoid neurons**\n",
    "\n",
    "## Working"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Perceptrons take several binary inputs and produces a single output.\n",
    "- Rosenblatt introduced a simple rule to compute the output\n",
    "- **Weights** - w1, w2,... are real numbers expressing the importance of the respective inputs to the output.\n",
    "- The neuron's output 0 or 1 is determined by whether the weighted sum $\\sum_j w_j x_j$ is less than or greater than some threshold value (real number, parameter of the neuron).\n",
    "i.e.\n",
    "\n",
    "$\\begin{eqnarray}\n",
    "  \\mbox{output} & = & \\left\\{ \\begin{array}{ll}\n",
    "      0 & \\mbox{if } \\sum_j w_j x_j \\leq \\mbox{ threshold} \\\\\n",
    "      1 & \\mbox{if } \\sum_j w_j x_j > \\mbox{ threshold}\n",
    "      \\end{array} \\right.\n",
    "\\tag{1}\\end{eqnarray}\n",
    "$\n",
    "\n",
    "> It is a device that makes decisions by weighing up evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E.g - Let's say we want to go to the park, but there are certain conditions which could influence the decision.\n",
    "Such as - Is the weather good? Is the park closed? Are my friends going?\n",
    "We can represent these binary factors with inputs as x1, x2, x3.\n",
    "Now, when the perceptron tries to make decision we emphasize that the weather is probably the most important factor in our decision making process and thus we give it a large weight value. Also, a threshold is chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplify the notations -\n",
    "\n",
    "- The weighted sum is changed to a matrix dot product, $w \\cdot x \\equiv \\sum_j w_j x_j$. Here, w and x are vectors whose components are the weights and inputs. \n",
    "- Also, the threshold is moved to the other side of the inequality. and is replaced by the  perceptron **bias**. $b \\equiv -\\mbox{threshold}$\n",
    "\n",
    "The new rules are\n",
    "\n",
    "$\n",
    "   \\begin{eqnarray}\n",
    "  \\mbox{output} = \\left\\{ \n",
    "    \\begin{array}{ll} \n",
    "      0 & \\mbox{if } w\\cdot x + b \\leq 0 \\\\\n",
    "      1 & \\mbox{if } w\\cdot x + b > 0\n",
    "    \\end{array}\n",
    "  \\right.\n",
    "\\tag{2}\\end{eqnarray}\n",
    "$\n",
    "\n",
    "- Bias is a measure of how easy it is to get the perceptron to output a 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptrons can also be used to compute elementary logical functions such as AND, OR and NAND.\n",
    "\n",
    "Most interesting thing is when a **learning algorithm** can tune the weights and biases in response to external stimuli without direct intervention. It can learn to solve problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid Neurons\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Small change in some weight or bias in the network. It will cause the output to change slightly. This is what makes the learning algorithms possible.\n",
    "\n",
    "- We can modify the weights and biases using the above to get the network to behave more in the manner we want.\n",
    "- The problem with perceptrons is that changing a weight or bias can completely flip the neuron output. This could have very different affects in the rest of the network. So, it is very difficult to gradually modify the weights and biases so that the network gets closer to the desired behavior.\n",
    "- This problem is solved by using a new type of neuron called a **Sigmoid neuron**.\n",
    "- They are similar to perceptrons only modified so that small change in weight or bias causes a small change in their output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working\n",
    "\n",
    "Similar to a perceptron, it has inputs x1, x2,... but instead of binary they can take any value between 0 and 1. It also has weights for each input and an overall bias, b.\n",
    "Also, the output is not 0 or 1. Instead it is  $\\sigma(w \\cdot x+b)$, where $\\sigma$ is called the sigmoid function and is defined by\n",
    "$\\begin{eqnarray} \n",
    "  \\sigma(z) \\equiv \\frac{1}{1+e^{-z}}.\n",
    "\\end{eqnarray}$\n",
    "\n",
    "Therefore, the outputs of sigmoid neuron is $\\begin{eqnarray} \n",
    "\\frac{1}{1+\\exp(-\\sum_j w_j x_j-b)}.\n",
    "\\end{eqnarray}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity to the perceptron model\n",
    "\n",
    "Suppose $z \\equiv w \\cdot x + b$ is a large positive value. Then $ e^{-z}\\approx 0 $ and so $\\sigma(z) \\approx 1$. If $z$ is large and positive, output of sigmoid neuron is 1, same like perceptrons. Similarly if $z$ is very negative, then $e^{-z} \\rightarrow \\infty$ and $\\sigma(z) \\approx 0$. Same as perceptrons.\n",
    "When the size of $z$ increases there's much deviation from the perceptron model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
